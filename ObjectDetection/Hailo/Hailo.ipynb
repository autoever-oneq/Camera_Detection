{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEcKFEDQ6xwZ"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zp6zedc4rB5"
      },
      "outputs": [],
      "source": [
        "# #Installing the python package\n",
        "# !pip install ultralytics --quiet\n",
        "# !pip install roboflow --quiet\n",
        "# !pip show ultralytics\n",
        "\n",
        "# #Verifying the installation\n",
        "# import ultralytics\n",
        "# from ultralytics import YOLO\n",
        "# from IPython.display import Image\n",
        "\n",
        "# ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nblhsfKK5TtD"
      },
      "outputs": [],
      "source": [
        "# # api_key는 계정마다 다름\n",
        "# from roboflow import Roboflow\n",
        "\n",
        "# rf = Roboflow(api_key=\"wNo4QrXGVHMuGz8nVoBq\")\n",
        "# project = rf.workspace(\"yolov8n-shyng\").project(\"one-q\")\n",
        "# version = project.version(1)\n",
        "# dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTrxXdeW5UYA"
      },
      "outputs": [],
      "source": [
        "# # Test(predict)\n",
        "# import os\n",
        "# model = YOLO(\"best.pt\")\n",
        "# # testImgDir = (f\"{dataset.location}/test/images\") # \"./ONE-Q-1/test/images\"\n",
        "\n",
        "# # print(testImgDir)\n",
        "# # for img in os.listdir(testImgDir):\n",
        "# #   imgName = f\"{testImgDir}/{img}\"\n",
        "# #   results = model.predict(source=imgName, save=True)\n",
        "# #   print(results[0].boxes.xyxy, results[0].boxes.xyxyn)\n",
        "# #   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCyuEkRW60sL"
      },
      "source": [
        "# ONNX 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAg1J2f563ot"
      },
      "outputs": [],
      "source": [
        "# !yolo export model=best.pt format=onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxgwTw5Y63d8"
      },
      "outputs": [],
      "source": [
        "# import onnx\n",
        "# import onnxruntime as ort\n",
        "# import torch\n",
        "\n",
        "# # Load the ONNX model\n",
        "# onnx_model = onnx.load(\"best.onnx\")\n",
        "# onnx.checker.check_model(onnx_model)\n",
        "# print(\"ONNX model is valid!\")\n",
        "\n",
        "# # Test the ONNX model with ONNX Runtime\n",
        "# # dummy_input = torch.randn(16, 3, 640, 640).numpy()\n",
        "# # ort_session = ort.InferenceSession(\"best.onnx\")\n",
        "# # outputs = ort_session.run(None, )\n",
        "# # print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TREvZyE5BtLS"
      },
      "source": [
        "# 가상환경 (전부)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQVvZvQ-73t6"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update --quiet\n",
        "!sudo apt-get install -y python3-dev python3-distutils python3-tk libfuse2 graphviz libgraphviz-dev --quiet\n",
        "\n",
        "# Will need a venv to install the DFC in\n",
        "!pip install --upgrade pip virtualenv --quiet\n",
        "#!virtualenv my_env\n",
        "!virtualenv --python=\"/usr/bin/python3.10\" my_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6IQxuRV9O5U"
      },
      "outputs": [],
      "source": [
        "#Installing the WHL file for Hailo DFC\n",
        "#!my_env/bin/pip install python==3.\n",
        "#!my_env/bin/pip install numpy==1.23.2\n",
        "!my_env/bin/pip install hailo_dataflow_compiler-3.30.0-py3-none-linux_x86_64.whl --quiet\n",
        "\n",
        "# Making sure it's installed properly\n",
        "!my_env/bin/hailo --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3721rku3RkD"
      },
      "source": [
        "# Step0. GPU setup (전부)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU3vfiYE-Y4i"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3i778Am-Yv-"
      },
      "outputs": [],
      "source": [
        "!sudo dpkg -i cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PIODgkYu-YqB"
      },
      "outputs": [],
      "source": [
        "!sudo cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-D95DBBE2-keyring.gpg /usr/share/keyrings/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8A8N5Gl-YjQ"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vl6P69g-YcL"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y cuda-11-8  # 입력 : 59 - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SllDx22j-YRN"
      },
      "outputs": [],
      "source": [
        "!echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc\n",
        "!echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\n",
        "!source ~/.bashrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSSfUB86-pfE"
      },
      "outputs": [],
      "source": [
        "!ls /usr/local | grep cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z0I_1Qz4-ruD"
      },
      "outputs": [],
      "source": [
        "!sudo ln -sf /usr/local/cuda-11.8 /usr/local/cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY8NU-sy-vj4"
      },
      "outputs": [],
      "source": [
        "!ls -l /usr/local/cuda\n",
        "!sudo update-alternatives --display cuda\n",
        "!sudo update-alternatives --config cuda\n",
        "# 이후 CUDA 11.8 버전에 해당하는 번호 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeH6aRePCd9L"
      },
      "outputs": [],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E8mliFg-xJc"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z8DYZKc3Y80"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y libnvinfer8=8.5.2-1+cuda11.8 libnvinfer-dev=8.5.2-1+cuda11.8 libnvinfer-plugin8=8.5.2-1+cuda11.8\n",
        "!apt-get install -y libcublas-11-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDM7_FCL3ahH"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!sudo ln -sf /usr/local/cuda-11.8 /usr/local/cuda\n",
        "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcudart.so /usr/lib/x86_64-linux-gnu/libcudart.so\n",
        "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcublas.so /usr/lib/x86_64-linux-gnu/libcublas.so\n",
        "!sudo ln -sf /usr/local/cuda-11.8/lib64/libcublasLt.so /usr/lib/x86_64-linux-gnu/libcublasLt.so\n",
        "!sudo apt-get remove --purge -y libnvinfer10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMDXX1Ln3ub-"
      },
      "outputs": [],
      "source": [
        "!dpkg -l | grep nvinfer\n",
        "!ls -l /usr/lib/x86_64-linux-gnu/libnvinfer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaA_qP8-3wIk"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get autoremove -y\n",
        "!sudo apt-get autoclean\n",
        "!sudo apt-get install -y libnvinfer8=8.5.2-1+cuda11.8 libnvinfer-dev=8.5.2-1+cuda11.8 libnvinfer-plugin8=8.5.2-1+cuda11.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hv38UGxA30Y_"
      },
      "outputs": [],
      "source": [
        "!echo 'export PATH=/usr/local/cuda-11.8/bin${PATH:+:${PATH}}' >> ~/.bashrc\n",
        "!echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc\n",
        "!source ~/.bashrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl2oZnaW312P"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCdlKDJZ36Zd"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/pip uninstall scipy --yes\n",
        "!my_env/bin/pip install scipy==1.9.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB3s7lNmLGOO"
      },
      "source": [
        "# Step1. Parsing ONNX file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFUxTcPwSdIu"
      },
      "source": [
        "## Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFgLCkBqLQa6"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/python translate_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbJeCWMkLVHr"
      },
      "source": [
        "### Script file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeWf9MWBJ9ed"
      },
      "outputs": [],
      "source": [
        "# from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# # Define the ONNX model path and configuration\n",
        "# onnx_path = \"best.onnx\"\n",
        "# onnx_model_name = \"yolov8n\"\n",
        "# chosen_hw_arch = \"hailo8\"  # Specify the target hardware architecture\n",
        "\n",
        "# # Initialize the ClientRunner\n",
        "# runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
        "\n",
        "# # Use the recommended end node names for translation\n",
        "# end_node_names = [\n",
        "#     \"/model.22/cv2.0/cv2.0.2/Conv\",  # P4 regression_layer\n",
        "#     \"/model.22/cv3.0/cv3.0.2/Conv\",  # P4 cls_layer\n",
        "#     \"/model.22/cv2.1/cv2.1.2/Conv\",  # P5 regression_layer\n",
        "#     \"/model.22/cv3.1/cv3.1.2/Conv\",  # P5 cls_layer,\n",
        "#     \"/model.22/cv2.2/cv2.2.2/Conv\",  # P6 regression_layer\n",
        "#     \"/model.22/cv3.2/cv3.2.2/Conv\",  # P6 cls_layer,\n",
        "# ]\n",
        "\n",
        "# try:\n",
        "#     # Translate the ONNX model to Hailo's format\n",
        "#     hn, npz = runner.translate_onnx_model(\n",
        "#         onnx_path,\n",
        "#         onnx_model_name,\n",
        "#         end_node_names=end_node_names,\n",
        "#         net_input_shapes={\"images\": [1, 3, 640, 640]},  # Adjust input shapes if needed\n",
        "#     )\n",
        "#     print(\"Model translation successful.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error during model translation: {e}\")\n",
        "#     raise\n",
        "\n",
        "# # Save the Hailo model HAR file\n",
        "# hailo_model_har_name = f\"{onnx_model_name}_hailo_model.har\"\n",
        "# try:\n",
        "#     runner.save_har(hailo_model_har_name)\n",
        "#     print(f\"HAR file saved as: {hailo_model_har_name}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error saving HAR file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUub5BzQShj1"
      },
      "source": [
        "# Step2. Optimizing model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBGZHApMTn9q"
      },
      "source": [
        "## Print Net Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4mALghnUrrZ"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/python print_net_info.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht5lOhVkTl8S"
      },
      "source": [
        "### Script file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEuuCUI_Sx4D"
      },
      "outputs": [],
      "source": [
        "# print_net_info.py\n",
        "# from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# # Load the HAR file\n",
        "# har_path = \"yolov8n_hailo_model.har\"\n",
        "\n",
        "# runner = ClientRunner(har=har_path)\n",
        "\n",
        "# from pprint import pprint\n",
        "\n",
        "# try:\n",
        "#     # Access the HailoNet as an OrderedDict\n",
        "#     hn_dict = runner.get_hn()  # Or use runner._hn if get_hn() is unavailable\n",
        "#     print(\"Inspecting layers from HailoNet (OrderedDict):\")\n",
        "\n",
        "#     # Pretty-print each layer\n",
        "#     for key, value in hn_dict.items():\n",
        "#         print(f\"Key: {key}\")\n",
        "#         pprint(value)\n",
        "#         print(\"\\n\" + \"=\"*80 + \"\\n\")  # Add a separator between layers for clarity\n",
        "\n",
        "# except Exception as e:\n",
        "#     print(f\"Error while inspecting hn_dict: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRHZdif_YBtU"
      },
      "source": [
        "## Jsonize (위 2개)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FDSs-7KYEyC"
      },
      "outputs": [],
      "source": [
        "#!pip install ultralytics --quiet\n",
        "!pip install roboflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s21e1CO5bRqc"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"wNo4QrXGVHMuGz8nVoBq\")\n",
        "project = rf.workspace(\"yolov8n-shyng\").project(\"one-q\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAVMQsKCYGRh"
      },
      "source": [
        "### Script file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a07e_HiNYJZc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Updated NMS layer configuration dictionary\n",
        "nms_layer_config = {\n",
        "\t\"nms_scores_th\": 0.2,\n",
        "\t\"nms_iou_th\": 0.7,\n",
        "\t\"image_dims\": [\n",
        "\t\t640,\n",
        "\t\t640\n",
        "\t],\n",
        "\t\"max_proposals_per_class\": 2,\n",
        "\t\"classes\": 2,\n",
        "\t\"regression_length\": 16,\n",
        "\t\"background_removal\": False,\n",
        "\t\"bbox_decoders\": [\n",
        "\t\t{\n",
        "\t\t\t\"name\": \"yolov8n/bbox_decoder41\",\n",
        "\t\t\t\"stride\": 8,\n",
        "\t\t\t\"reg_layer\": \"yolov8n/conv41\",\n",
        "\t\t\t\"cls_layer\": \"yolov8n/conv42\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"name\": \"yolov8n/bbox_decoder52\",\n",
        "\t\t\t\"stride\": 16,\n",
        "\t\t\t\"reg_layer\": \"yolov8n/conv52\",\n",
        "\t\t\t\"cls_layer\": \"yolov8n/conv53\"\n",
        "\t\t},\n",
        "\t\t{\n",
        "\t\t\t\"name\": \"yolov8n/bbox_decoder62\",\n",
        "\t\t\t\"stride\": 32,\n",
        "\t\t\t\"reg_layer\": \"yolov8n/conv62\",\n",
        "\t\t\t\"cls_layer\": \"yolov8n/conv63\"\n",
        "\t\t}\n",
        "\t]\n",
        "}\n",
        "\n",
        "# Path to save the updated JSON configuration\n",
        "output_dir = \"/save/path/\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "output_path = os.path.join(output_dir, \"nms_layer_config.json\")\n",
        "\n",
        "# Save the updated configuration as a JSON file\n",
        "with open(output_path, \"w\") as json_file:\n",
        "    json.dump(nms_layer_config, json_file, indent=4)\n",
        "\n",
        "print(f\"NMS layer configuration saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2roUWBWuZ4z"
      },
      "source": [
        "## Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pueyXBK18esk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ7vhoP9yeEX"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/python optimize_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh4dv5tFudOB"
      },
      "source": [
        "### Script file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzC3E288ZbLy"
      },
      "outputs": [],
      "source": [
        "### BELOW CODE CANNOT RUN ON COLAB(RAM SHUTDOWN) -> Run in HAILO HW ###\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Mounting Google Drive\n",
        "# drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# # Paths to directories and files\n",
        "# image_dir = 'ONE-Q-1/train/images'\n",
        "# output_dir = '/path/to/output_dir'\n",
        "# os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# # File paths for saving calibration data\n",
        "# calibration_data_path = os.path.join(output_dir, \"calibration_data.npy\")\n",
        "# processed_data_path = os.path.join(output_dir, \"processed_calibration_data.npy\")\n",
        "\n",
        "# # Initialize an empty list for calibration data\n",
        "# calib_data = []\n",
        "\n",
        "# # Process all image files in the directory\n",
        "# for img_name in os.listdir(image_dir):\n",
        "#     img_path = os.path.join(image_dir, img_name)\n",
        "#     if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "#         img = Image.open(img_path).resize((640, 640))  # Resize to desired dimensions\n",
        "#         img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "#         calib_data.append(img_array)\n",
        "\n",
        "# # Convert the calibration data to a NumPy array\n",
        "# calib_data = np.array(calib_data)\n",
        "\n",
        "# # Save the normalized calibration data\n",
        "# np.save(calibration_data_path, calib_data)\n",
        "# print(f\"Normalized calibration dataset saved with shape: {calib_data.shape} to {calibration_data_path}\")\n",
        "\n",
        "# # Scale the normalized data back to [0, 255]\n",
        "# processed_calibration_data = calib_data * 255.0\n",
        "\n",
        "# # Save the processed calibration data\n",
        "# np.save(processed_data_path, processed_calibration_data)\n",
        "# print(f\"Processed calibration dataset saved with shape: {processed_calibration_data.shape} to {processed_data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_SjcYo4dVsb"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# # Define your model's HAR file name\n",
        "# model_name = \"yolov8n\"\n",
        "# hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
        "\n",
        "# # Ensure the HAR file exists\n",
        "# assert os.path.isfile(hailo_model_har_name), \"Please provide a valid path for the HAR file\"\n",
        "\n",
        "# # Initialize the ClientRunner with the HAR file\n",
        "# runner = ClientRunner(har=hailo_model_har_name)\n",
        "\n",
        "# # Define the model script to add a normalization layer\n",
        "# # Normalization for [0, 1] range\n",
        "# alls = \"\"\"\n",
        "# normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\n",
        "# change_output_activation(conv42, sigmoid)\n",
        "# change_output_activation(conv53, sigmoid)\n",
        "# change_output_activation(conv63, sigmoid)\n",
        "# nms_postprocess(\"/content/nms_layer_config.json\", meta_arch=yolov8n, engine=cpu)\n",
        "# performance_param(compiler_optimization_level=max)\n",
        "# \"\"\"\n",
        "\n",
        "# # Load the model script into the ClientRunner\n",
        "# runner.load_model_script(alls)\n",
        "\n",
        "# # Define a calibration dataset\n",
        "# # Replace 'calib_dataset' with the actual dataset you're using for calibration\n",
        "# # For example, if it's a directory of images, prepare the dataset accordingly\n",
        "# calib_dataset = \"/content/processed_calibration_data.npy\"\n",
        "\n",
        "# # Perform optimization with the calibration dataset\n",
        "# runner.optimize(calib_dataset)\n",
        "\n",
        "# # Save the optimized model to a new Quantized HAR file\n",
        "# quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
        "# runner.save_har(quantized_model_har_path)\n",
        "\n",
        "# print(f\"Quantized HAR file saved to: {quantized_model_har_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pYmjs5JdilJ"
      },
      "source": [
        "# Step3. Compile model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWxFxUOVzg6k"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pBf00OazjYD"
      },
      "outputs": [],
      "source": [
        "# !my_env/bin/python compile_model.py\n",
        "!my_env/bin/python compile_model_main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HodBej23zrkQ"
      },
      "source": [
        "### Script file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uYLFhOeds_k"
      },
      "outputs": [],
      "source": [
        "# from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# # Define the quantized model HAR file\n",
        "# model_name = \"yolov8n\"\n",
        "# quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
        "\n",
        "# # Initialize the ClientRunner with the HAR file\n",
        "# runner = ClientRunner(har=quantized_model_har_path)\n",
        "# print(\"[info] ClientRunner initialized successfully.\")\n",
        "\n",
        "# # Compile the model\n",
        "# try:\n",
        "#     hef = runner.compile()\n",
        "#     print(\"[info] Compilation completed successfully.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"[error] Failed to compile the model: {e}\")\n",
        "#     raise\n",
        "# file_name = f\"{model_name}.hef\"\n",
        "# with open(file_name, \"wb\") as f:\n",
        "#     f.write(hef)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import sys\n",
        "# import logging\n",
        "# import psutil\n",
        "# import time\n",
        "# import datetime\n",
        "# from threading import Thread\n",
        "# from pathlib import Path\n",
        "# from IPython.display import clear_output, display, Image\n",
        "# import subprocess\n",
        "# import queue\n",
        "# import shutil\n",
        "# import glob\n",
        "\n",
        "# # Configuración de directorios y logs\n",
        "# class Config:\n",
        "#     def __init__(self):\n",
        "#         self.log_dir = Path('log')\n",
        "#         self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "#         self.plots_dir = self.log_dir / f'plots_{self.timestamp}'\n",
        "#         self.log_file = self.log_dir / f'compilation_log_{self.timestamp}.txt'\n",
        "\n",
        "#         # Crear directorios necesarios\n",
        "#         self.log_dir.mkdir(exist_ok=True)\n",
        "#         self.plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "#         # Configurar logging\n",
        "#         logging.basicConfig(\n",
        "#             level=logging.INFO,\n",
        "#             format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "#             handlers=[\n",
        "#                 logging.FileHandler(self.log_file),\n",
        "#                 logging.StreamHandler()\n",
        "#             ]\n",
        "#         )\n",
        "\n",
        "# class ResourceMonitor:\n",
        "#     def __init__(self):\n",
        "#         self.config = Config()\n",
        "#         self.compile_output = []\n",
        "\n",
        "# 하단 골뱅이 하나 지워야함\n",
        "#     @@staticmethod\n",
        "#     def format_bytes(bytes_value):\n",
        "#         for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "#             if bytes_value < 1024:\n",
        "#                 return f\"{bytes_value:.2f}{unit}\"\n",
        "#             bytes_value /= 1024\n",
        "#         return f\"{bytes_value:.2f}TB\"\n",
        "\n",
        "#     def log_output(self, line):\n",
        "#         with open(self.config.log_file, 'a', encoding='utf-8') as f:\n",
        "#             timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "#             f.write(f\"[{timestamp}] {line}\\n\")\n",
        "\n",
        "#     def create_progress_bar(self, percentage, length=25):\n",
        "#         filled = int(length * percentage / 100)\n",
        "#         return f\"{'█' * filled}{'░' * (length - filled)}\"\n",
        "\n",
        "#     def print_status(self, cpu, mem, disk):\n",
        "#         clear_output(wait=True)\n",
        "#         current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "#         # Cabecera\n",
        "#         print(f\"\\n{'═' * 50}\")\n",
        "#         print(f\"  MONITOR DE RECURSOS - {current_time}\")\n",
        "#         print(f\"{'═' * 50}\\n\")\n",
        "\n",
        "#         # Estado de Compilación\n",
        "#         if self.compile_output:\n",
        "#             print(\"ESTADO DE COMPILACIÓN (últimas 5 líneas):\")\n",
        "#             print(f\"{'─' * 50}\")\n",
        "#             for line in self.compile_output[-5:]:\n",
        "#                 print(f\"  {line}\")\n",
        "#             print(f\"{'─' * 50}\\n\")\n",
        "\n",
        "#         # Uso de Recursos\n",
        "#         print(\"USO DE RECURSOS:\")\n",
        "#         print(f\"CPU:  {self.create_progress_bar(cpu)} {cpu:>5.1f}%\")\n",
        "#         print(f\"RAM:  {self.create_progress_bar(mem.percent)} {mem.percent:>5.1f}%\")\n",
        "#         print(f\"DISK: {self.create_progress_bar(disk.percent)} {disk.percent:>5.1f}%\")\n",
        "\n",
        "#         # Detalles de Memoria\n",
        "#         print(\"\\nDETALLES DE MEMORIA:\")\n",
        "#         print(f\"  Total: {self.format_bytes(mem.total):>10}\")\n",
        "#         print(f\"  Usado: {self.format_bytes(mem.used):>10}\")\n",
        "#         print(f\"  Libre: {self.format_bytes(mem.available):>10}\")\n",
        "\n",
        "#     def monitor_resources(self):\n",
        "#         while True:\n",
        "#             try:\n",
        "#                 cpu = psutil.cpu_percent(interval=1)\n",
        "#                 mem = psutil.virtual_memory()\n",
        "#                 disk = psutil.disk_usage('/')\n",
        "#                 self.print_status(cpu, mem, disk)\n",
        "#             except Exception as e:\n",
        "#                 logging.error(f\"Error en monitoreo: {str(e)}\")\n",
        "#             time.sleep(2)\n",
        "\n",
        "#     def read_output(self, pipe, queue):\n",
        "#         try:\n",
        "#             for line in pipe:\n",
        "#                 line = line.strip()\n",
        "#                 if line:  # Solo procesar líneas no vacías\n",
        "#                     queue.put(line)\n",
        "#                     self.log_output(line)\n",
        "#                     self.compile_output.append(line)\n",
        "#                     if len(self.compile_output) > 100:  # Mantener solo últimas 100 líneas\n",
        "#                         self.compile_output.pop(0)\n",
        "#         finally:\n",
        "#             if hasattr(pipe, 'close'):\n",
        "#                 pipe.close()\n",
        "\n",
        "#     def run(self):\n",
        "#         try:\n",
        "#             print(\"\\n=== INICIANDO MONITOREO DE SISTEMA ===\")\n",
        "#             print(f\"Logs: {self.config.log_file}\")\n",
        "#             print(f\"Plots: {self.config.plots_dir}\\n\")\n",
        "\n",
        "#             output_queue = queue.Queue()\n",
        "\n",
        "#             # Iniciar thread de monitoreo\n",
        "#             monitor_thread = Thread(target=self.monitor_resources, daemon=True)\n",
        "#             monitor_thread.start()\n",
        "\n",
        "#             # Configurar y ejecutar proceso de compilación\n",
        "#             env_path = Path('my_env/bin/python')\n",
        "#             compile_script = Path('compile_model.py')\n",
        "\n",
        "#             if not env_path.exists():\n",
        "#                 raise FileNotFoundError(f\"Entorno virtual no encontrado en {env_path}\")\n",
        "#             if not compile_script.exists():\n",
        "#                 raise FileNotFoundError(f\"Script no encontrado en {compile_script}\")\n",
        "\n",
        "#             process = subprocess.Popen(\n",
        "#                 [str(env_path), str(compile_script)],\n",
        "#                 stdout=subprocess.PIPE,\n",
        "#                 stderr=subprocess.PIPE,\n",
        "#                 text=True,\n",
        "#                 bufsize=1\n",
        "#             )\n",
        "\n",
        "#             # Iniciar threads para stdout y stderr\n",
        "#             for pipe in [process.stdout, process.stderr]:\n",
        "#                 Thread(target=self.read_output,\n",
        "#                       args=(pipe, output_queue),\n",
        "#                       daemon=True).start()\n",
        "\n",
        "#             # Esperar a que termine el proceso\n",
        "#             return_code = process.wait()\n",
        "\n",
        "#             if return_code != 0:\n",
        "#                 print(f\"\\nError en la compilación (código {return_code})\")\n",
        "#             else:\n",
        "#                 print(\"\\nCompilación completada exitosamente\")\n",
        "\n",
        "#         except KeyboardInterrupt:\n",
        "#             print(\"\\nDetención manual del monitoreo\")\n",
        "#             if 'process' in locals():\n",
        "#                 process.terminate()\n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error en la ejecución: {str(e)}\")\n",
        "#             raise\n",
        "#         finally:\n",
        "#             print(f\"\\nLogs guardados en: {self.config.log_file}\")\n",
        "#             print(f\"Gráficos guardados en: {self.config.plots_dir}\")\n",
        "#             sys.exit(0)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     monitor = ResourceMonitor()\n",
        "#     monitor.run()"
      ],
      "metadata": {
        "id": "CKsn99ipuG0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pEcKFEDQ6xwZ",
        "lCyuEkRW60sL",
        "TREvZyE5BtLS",
        "J3721rku3RkD",
        "NB3s7lNmLGOO",
        "sFUxTcPwSdIu",
        "NbJeCWMkLVHr",
        "YBGZHApMTn9q",
        "Ht5lOhVkTl8S",
        "IRHZdif_YBtU",
        "IAVMQsKCYGRh",
        "HodBej23zrkQ"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}